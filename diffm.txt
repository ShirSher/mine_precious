8,9c8,9
<     1)draw two images for MINE
<     2)create joint_distribution and marginal distribution 
---
>     1)drew two images for MINE
>     2)create joint_disterbution and marginal distribution 
12c12,14
<         
---
> 
> In this implimintation we compare the test and train set in order to see
> coherence of results and to point where the algorithm overfits       
21,24d22
< 
< import matplotlib.pyplot as plt
< 
< 
26,27d23
< import torch.nn as nn
< import torch.nn.functional as F
30d25
< import torchvision
34d28
< import FullDriftDataset
37,38c31
< # mod added traject_input_dim and hardcoded to dimenstions ()
< # mod rm "net_num", "traject", "dataset_status"
---
> 
40c33
<     def __init__(self, traject_input_dim=[14,300], train = True,batch = 1000, lr = 3e-3, gamma = 0.001, optimizer=2,
---
>     def __init__(self, train = True,traject = True, batch = 1000, lr = 3e-3, gamma = 0.001, optimizer=2, net_num = 3,
44,48c37,42
<                  number_repeating_blocks=0, repeating_blockd_size=512):
<         # self.net_num = net_num
<         # self.traject = traject
<         # self.dataset_status = dataset_status
<         # print(self.dataset_status)
---
>                  number_repeating_blocks=0, repeating_blockd_size=512,
>                  dataset_status = 'same'):
>         self.net_num = net_num
>         self.traject = traject
>         self.dataset_status = dataset_status
>         print(self.dataset_status)
58,62c52,55
<         # mod
<         self.traject_input_dim = traject_input_dim
<         #Defining the network:
<         self.net = networks.statistical_estimator(traject_max_depth = self.traject_max_depth,
<                                                     traject_input_dim= self.traject_input_dim,
---
>         #Defining the netwrok:
>         if self.traject == 'combined':
>             print('MNIST Trajectory MINE network')
>             self.net = networks.statistical_estimator(traject_max_depth = self.traject_max_depth,
70a64,89
>         elif self.traject == 'traject':
>             print('Traject MINE network')
>             self.net = networks.conv1d_classifier_(input_dim=[18,1000,0], output_size = 1,
>                                                    p_conv = 0, p_fc = 0, 
>                                                    max_depth = self.traject_max_depth, 
>                                                    num_layers = self.traject_num_layers, 
>                                                    conv_depth_type = 'decending',
>                                                    repeating_block_depth = 5, 
>                                                    repeating_block_size = 0,
>                                                    stride = self.traject_stride, 
>                                                    kernel = self.traject_kernel, 
>                                                    padding = self.traject_padding,
>                                                    pooling = self.traject_pooling, 
>                                                    BN = False)
>         else:
>             print('MNIST MINE network')
>             if self.net_num == 1:
>                 self.net = networks.statistical_estimator_DCGAN(input_size = 2, output_size = 1)
>             elif self.net_num == 2:
>                 self.net = networks.statistical_estimator_DCGAN_2(input_size = 1, output_size = 1)
>             else:
>                 self.net = networks.statistical_estimator_DCGAN_3(input_size = 1, output_size = 1,number_descending_blocks = self.number_descending_blocks, 
>                      number_repeating_blocks=self.number_repeating_blocks, repeating_blockd_size = self.repeating_blockd_size)
>         
>         #self.input1 = input1
>         #self.input2 = input2
87c106
<         #MOD TODO train/test
---
>         
101,105c120,124
<         # print('Using Net {}'.format(self.net_num))
<         # if self.net_num == 3:
<         #     print('Number of Descending Blocks is {}'.format(self.number_descending_blocks))
<         #     print('Number of times to repeat a block = {}'.format(self.number_repeating_blocks))
<         #     print('The fully connected layer to repeat - {}'.format(self.repeating_blockd_size))
---
>         print('Using Net {}'.format(self.net_num))
>         if self.net_num == 3:
>             print('Number of Descending Blocks is {}'.format(self.number_descending_blocks))
>             print('Number of times to repeat a block = {}'.format(self.number_repeating_blocks))
>             print('The fully connected layer to repeat - {}'.format(self.repeating_blockd_size))
108,112c127,150
<         self.net = networks.statistical_estimator(traject_max_depth = self.traject_max_depth,
<                 traject_num_layers = self.traject_num_layers, traject_stride = self.traject_stride,
<                 traject_kernel = self.traject_kernel, traject_padding = self.traject_padding,
<                 traject_pooling = self.traject_pooling, number_descending_blocks=self.number_descending_blocks, 
<                 number_repeating_blocks = self.number_repeating_blocks, repeating_blockd_size = self.repeating_blockd_size)
---
> 
>         if self.traject == 'combine':
>             print('MNIST Trajectory MINE network')
>             self.net = networks.statistical_estimator(traject_max_depth = self.traject_max_depth,
>                  traject_num_layers = self.traject_num_layers, traject_stride = self.traject_stride,
>                  traject_kernel = self.traject_kernel, traject_padding = self.traject_padding,
>                  traject_pooling = self.traject_pooling, number_descending_blocks=self.number_descending_blocks, 
>                  number_repeating_blocks = self.number_repeating_blocks, repeating_blockd_size = self.repeating_blockd_size)
>         elif self.traject == 'traject':
>             print('Traject MINE network')
>             self.net = networks.conv1d_classifier_(input_dim=[18,1000,0], output_size = 1,
>                                                    max_depth = self.traject_max_depth, 
>                                                    p_conv = 0, p_fc = 0, BN = False)
>         else:
>             print('MNIST MINE network')
>             if self.net_num == 1:
>                 self.net = networks.statistical_estimator_DCGAN(input_size = 2, output_size = 1)
>             elif self.net_num == 2:
>                 self.net = networks.statistical_estimator_DCGAN_2(input_size = 1, output_size = 1)
>             else:
>                 self.net = networks.statistical_estimator_DCGAN_3(input_size = 1, output_size = 1,number_descending_blocks = self.number_descending_blocks, 
>                      number_repeating_blocks=self.number_repeating_blocks, repeating_blockd_size = self.repeating_blockd_size)
>             
>         print('')
127c165,169
<        
---
>         print('Using Net {}'.format(self.net_num))
>         if self.net_num == 3:
>             print('Number of Descending Blocks is {}'.format(self.number_descending_blocks))
>             print('Number of times to repeat a block = {}'.format(self.number_repeating_blocks))
>             print('The fully connected layer to repeat - {}'.format(self.repeating_blockd_size))
129,138c171,180
<     def mutual_information(self,joint1, joint2, marginal):
<         # TODO Confirm non "traject"
<         # if self.traject == 'traject':
<         #     obj = torch.cat((joint1,joint2),1)
<         #     T = self.net(obj)
<         #     obj2 = torch.cat((joint1, marginal),1)
<         #     eT = torch.exp(self.net(obj2))
<         # else:
<         T = self.net(joint1, joint2)
<         eT = torch.exp(self.net(joint1, marginal))
---
>             
>     def mutual_information(self, joint1, joint2, marginal,train = True):
>         if self.traject == 'traject':
>             obj = torch.cat((joint1,joint2),1)
>             T = self.net(obj)
>             obj2 = torch.cat((joint1, marginal),1)
>             eT = torch.exp(self.net(obj2))
>         else:
>             T = self.net(joint1, joint2)
>             eT = torch.exp(self.net(joint1, marginal))
141,144d182
< # MOD
<     def validate_mine(self, batch,ma_rate=0.01):
<         # batch is a tuple of (joint1, joint2, marginal (from the dataset of joint 2))
<         joint1, joint2, marginal = batch 
146,178c184
<         # joint1 = torch.autograd.Variable(batch[0])
<         # joint2 = torch.autograd.Variable(batch[1])
<         # marginal = torch.autograd.Variable(batch[2]) #the uneven parts of the dataset are the labels 
<         # if torch.cuda.is_available():
<         #     joint1 = joint1.to('cuda', non_blocking=True)
<         #     joint2 = joint2.to('cuda', non_blocking=True)
<         #     marginal = marginal.to('cuda', non_blocking=True)
<         #     self.net = self.net.cuda()
<         #joint = torch.autograd.Variable(torch.FloatTensor(joint))
<         #marginal = torch.autograd.Variable(torch.FloatTensor(marginal))
<         
<         NIM , T, eT = self.mutual_information(joint1, joint2, marginal)
<         
<         #Using exponantial moving average to correct bias 
<         ma_eT = (1-ma_rate)*eT + (ma_rate)*torch.mean(eT) 
<         # unbiasing 
<         loss = -(torch.mean(T) - (1/ma_eT.mean()).detach()*torch.mean(eT))
<         # use biased estimator
<         # loss = - mi_lb
<         
<         # self.mine_net_optim.zero_grad()
<         # autograd.backward(loss)
<         # self.mine_net_optim.step()
<         #self.scheduler.step()
<         #self.scheduler2.step(NIM)
<         # if cuda, why put it on the cpu?
<         if torch.cuda.is_available():
<             NIM = NIM.cpu() 
<             loss = loss.cpu()
<         return NIM, loss
< 
< 
<     def learn_mine(self,batch, ma_rate=0.01):
---
>     def learn_mine(self,batch, ma_rate=0.01, train = True):
180,190c186,193
<         joint1, joint2, marginal = batch 
<         # joint1 = torch.autograd.Variable(batch[0])
<         # mod from [2] to [1]
<         # joint2 = torch.autograd.Variable(batch[1])
<         # mod from [4] to [2]
<         # marginal = torch.autograd.Variable(batch[2]) #the uneven parts of the dataset are the labels 
<         # if torch.cuda.is_available():
<             # joint1 = joint1.to('cuda', non_blocking=True)
<             # joint2 = joint2.to('cuda', non_blocking=True)
<             # marginal = marginal.to('cuda', non_blocking=True)
<             # self.net = self.net.cuda()
---
>         joint1 = torch.autograd.Variable(batch[0])
>         joint2 = torch.autograd.Variable(batch[2])
>         marginal = torch.autograd.Variable(batch[4]) #the uneven parts of the dataset are the labels 
>         if torch.cuda.is_available():
>             joint1 = joint1.to('cuda', non_blocking=True)
>             joint2 = joint2.to('cuda', non_blocking=True)
>             marginal = marginal.to('cuda', non_blocking=True)
>             self.net = self.net.cuda()
194c197
<         NIM , T, eT = self.mutual_information(joint1, joint2, marginal)
---
>         NIM , T, eT = self.mutual_information(joint1, joint2, marginal, train = train)
202,205c205,208
<         
<         self.mine_net_optim.zero_grad()
<         autograd.backward(loss)
<         self.mine_net_optim.step()
---
>         if train:
>             self.mine_net_optim.zero_grad()
>             autograd.backward(loss)
>             self.mine_net_optim.step()
208d210
<         # if cuda, why put it on the cpu?
213,214c215,216
<     # mod insert ix_selection
<     def epoch(self,ix_selection,num_epoch = 1):
---
>     
>     def epoch(self,num_epoch = 1):
215a218,219
>         
>         
216a221
>         test_results = list()
219,220c224,234
<         dataset = FullDriftDataset.FullDataset(ix_dict=ix_selection)
<         dataloader = torch.utils.data.DataLoader(dataset,  batch_size = self.batch_size, shuffle = True)    
---
>         if self.traject == 'combined':
>             dataset = utils.MNIST_TRAJECT_MINE(dataset_status = self.dataset_status)
>             dataset_test = utils.MNIST_TRAJECT_MINE(dataset_status = self.dataset_status, train = False)
>         elif self.traject == 'traject':
>             dataset = utils.TRAJECT_MINE2()
>         else:
>             dataset = utils.MNIST_for_MINE(train=self.train_value, trans = None)
>             dataset_test = utils.MNIST_for_MINE(train=self.train_value, trans = None)
>         
>         dataloader = torch.utils.data.DataLoader(dataset,  batch_size = self.batch_size, shuffle = True)
>         dataloader_test = torch.utils.data.DataLoader(dataset_test,  batch_size = self.batch_size, shuffle = True)
233a248,263
>         
>         if num_epoch%100 == 0:
>             temp_results = []
>             for idx, batch in enumerate(dataloader_test):
>                 NIM, loss = self.learn_mine(batch)
>                 temp_results.append(NIM.detach())
>                 if torch.isnan(temp_results[-1]):
>                     print(temp_results[-6:-1])
>                     print('Got to NaN in epoch {0} batch {1}'.format(num_epoch,idx))
>                     #plt.plot(result)
>                     nan = True
>                     break
>                 #bar.update()
>                 test_results.append(np.mean(temp_results))
>             test_results = [np.mean(test_results)]
>         
235c265
<         return np.mean(result), nan
---
>         return np.mean(result), nan, test_results
238a269
>         test_results = []
241c272
<             result, nan = self.epoch(epoch)
---
>             result, nan, test_results = self.epoch(epoch)
245a277,278
>             if len(test_results) == 1:
>                 test_results.append(test_results[0])
248a282
>         self.test_results.append(test_results)
252c286,291
<         return [np.mean(a[i:i+window_size]) for i in range(0,len(a)-window_size)]
\ No newline at end of file
---
>         return [np.mean(a[i:i+window_size]) for i in range(0,len(a)-window_size)]
> 
> 
>         
>     
> 
