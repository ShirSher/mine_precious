import torch
import torchvision
import numpy as np
import pandas as pd
import os
import pickle
from datetime import datetime
from random import shuffle, choice
import utils
import numpy as np
from scipy.fft import fft2, fftshift

# norms[55] = "[['0.4281566203536026', '0.4233150426674287', '0.4227252447683038', '0.42093985370370063', '0.41716957339226945', '0.4171815083075229', '0.41682226940513745', '0.4121999231084266', '0.41304225288393936', '0.40839529177967504', '0.40916069851862646', '0.4107023153069885', '0.40689880602225337', '0.4069763704387797', '0.406393221564715', '0.4056943626061281', '0.4052521891475206', '0.40607719603384024', '0.4060455998306377', '0.4046035756288934', '0.4025702516583264', '0.4028774724513301', '0.4033850422944867', '0.4035919370660463', '0.40172938545725667', '0.39825520483570065', '0.39581942872709314', '0.39393622633582287', '0.39331220341798073', '0.3923405117768756', '0.39326883844419813', '0.3914826275309983', '0.3893029104653765', '0.38871594667639986', '0.389642108735447', '0.3883863671095929', '0.3871845998504767', '0.3860777191821789', '0.38403884232843977', '0.38327577985462513', '0.38107414957541025', '0.3817600852679404', '0.3818340520069061', '0.3811174188776211', '0.37903615509643285', '0.38017922284555467', '0.38064665853875834', '0.38493255078778893', '0.3844343693795714', '0.38401733301394325', '0.38513550372541844', '0.38404259287560716', '0.38457058083918405', '0.38408574717041094', '0.3838916778777436', '0.3844207604888724', '0.3842670774899513', '0.38521433448267456', '0.3855335185977441', '0.3871745602290184', '0.38890552213504187', '0.38715163696352123', '0.387108450303396', '0.38525995193439533', '0.3852594473122978', '0.3836545114369471', '0.3815959830729683', '0.38012285979470223', '0.3773612601826365', '0.37681827117637734', '0.3788096827976933', '0.38003453405352705', '0.38136530661412044', '0.3832588915937083', '0.38347157779865987', '0.38241068789714755', '0.38257807940859934', '0.3811721619225651', '0.3776229846540078', '0.37719133149040085', '0.37815143179452865', '0.37619062105489276', '0.3754885770626185', '0.3741282283525594', '0.37455126770046854', '0.3729194146583787', '0.37410910277085696', '0.376206091629828', '0.3756758534995591', '0.3743257391799232', '0.37602663415897203', '0.3752138939072041', '0.3754049736705074', '0.37663907475057423', '0.37615071663061467', '0.37716760445331554', '0.3782748954122253', '0.3817333483741351', '0.380131397627299', '0.37909733716156846', '0.3809215971980864', '0.3802555568697071', '0.3792862511393973', '0.38040953899844765', '0.382638252920407', '0.38398991540376193', '0.38443066985614294', '0.38332694965640124', '0.37982607470231033', '0.3790543492751892', '0.37926217544425206', '0.3802126518797483', '0.38232538214108935', '0.38054255183838237', '0.38043365231068726', '0.38052049548586486', '0.3816556251597357', '0.3814175429098867', '0.38191690763987995', '0.3818918465067578', '0.3827909130778664', '0.384394016246044', '0.38377399417149716', '0.3836913292509515', '0.3819976659143561', '0.3816838980032527', '0.38305496148338997', '0.3814250087163515', '0.38182809305182247', '0.38545622840338056', '0.3844049672223191', '0.3854535396499166', '0.3867880243567575', '0.38777439354351273', '0.38969554361950465', '0.38921476882778866', '0.38986558033398994', '0.39157070328104804', '0.3943191482542906', '0.3948702178801597', '0.39444831877465714', '0.39865696906108755', '0.3991191152220486', '0.39923780758482186', '0.4015553927843669', '0.4015166532933142', '0.40196153614163804', '0.40210526038367733', '0.40229004508631283', '0.4010982601879675', '0.401564073088961', '0.4016238385522405', '0.39934088640201415', '0.39722154484124356', '0.3970193600660747', '0.3963403512155358', '0.3973332552780155', '0.3975674562408208', '0.39577961868611566', '0.3946620356341467', '0.39485157162173384', '0.3911061063122484', '0.39323034323189043', '0.3927915651950218', '0.393391109130103', '0.3920389511313843', '0.3899805009799744', '0.3920238755574009', '0.3911438062825853', '0.3924037463465474', '0.39045083510667566', '0.38847292123884125', '0.3901280559684025', '0.39155168257029754', '0.3899105574123397', '0.39012381716597566', '0.3927723837591278', '0.3915104381790163', '0.39210826101008667', '0.39277823113472693', '0.39252603981761436', '0.3925069575595357', '0.39121995887995586', '0.3897355564554921', '0.39088405591257536', '0.39083899961386426', '0.38921697360462515', '0.3894877239645319', '0.3918334617534339', '0.3909141743726125', '0.3912119324638931', '0.39026215822874344', '0.38963185666077826', '0.3876411312767571', '0.38668298027852854', '0.38699420153599706', '0.3856264067262102', '0.3875176297519901', '0.3885117091418537', '0.3884488197394974', '0.38874290789284344', '0.38722857764077767', '0.38625690385921896', '0.3855439879996113', '0.3863262248696473', '0.385855330666562', '0.38536987978192344', '0.3830215314481931', '0.38462707659323475', '0.3870606919400985', '0.38727838135734577', '0.38767094615377884', '0.387714259045241', '0.42362277902018486', '0.3877101256817139', '0.38853262503072805', '0.3890512849259624', '0.38908910052678636', '0.38984830558030054', '0.38990843120136265', '0.35216097217872744', '0.38893317971110164', '0.3912675859230291', '0.3917837002073431', '0.3903821871335669', '0.3908782430227077', '0.39145075993948525', '0.39040640405378807', '0.38915279972764527', '0.3891582703751211', '0.388869181546945', '0.3887191592614472', '0.3887430664808872', '0.38993187854129385', '0.389383177007976', '0.3897959535552792', '0.39162037090415014', '0.3898938466036298', '0.3915337955553301', '0.39339887161773784'], 
#      ['0.4756762111620909', '0.4818887306161722', '0.4813825866172555', '0.49257750757720264', '0.4999176565690605', '0.5006071759943278', '0.4962635003599831', '0.5001009348049698', '0.5043441770713704', '0.5068673773787054', '0.5083579032311198', '0.5081767541314431', '0.5131854849049386', '0.5176465149713159', '0.5194744408437104', '0.5185474576098705', '0.5154108115671079', '0.5153957435997256', '0.5165827043437369', '0.5173038547457169', '0.5181436138325052', '0.5197411539909026', '0.5168571277769793', '0.5198360771232196', '0.5196823505991753', '0.5229118579752718', '0.5221201602587373', '0.525919334966729', '0.523255957401418', '0.5247022025207302', '0.5273995455878431', '0.528969449786977', '0.5312485275319727', '0.5294190064230494', '0.5288057022965649', '0.5309923501840359', '0.5326144345483038', '0.5340225198027189', '0.5339744747592603', '0.5360428760278286', '0.5377988334338031', '0.5373159425586003', '0.5401364520263474', '0.5415704826785346', '0.5432834718917983', '0.5409230334690247', '0.5393206560623971', '0.5371000582033548', '0.5346757101048478', '0.5324391383949801', '0.5310457493762815', '0.5348695521197632', '0.5342597224760415', '0.533512441629771', '0.5346897188775198', '0.5343568834688055', '0.5353156871296998', '0.538343874635937', '0.5374022910768829', '0.5369393146201622', '0.5318613644271633', '0.5312514605829982', '0.5301607284855653', '0.5286459928833798', '0.5300634320044746', '0.5252918178177439', '0.5233296921228131', '0.5220999534862845', '0.5234650433035932', '0.5253044685882657', '0.5242576895645551', '0.5224393427644902', '0.5187373803039615', '0.5173211450806146', '0.5156965775486105', '0.5164527242967809', '0.5144488548471036', '0.5116630892368308', '0.512074609731627', '0.5106215740407113', '0.5104315611375105', '0.5168447380810328', '0.5170509050177889', '0.5179550827893675', '0.5185616152156692', '0.5216463356949375', '0.5256570176631932', '0.528268762645382', '0.5285017006997426', '0.5266363524395749', '0.5270022582128198', '0.527178095621116', '0.5283313811221749', '0.5281293109829376', '0.5306416629608324', '0.5295630024643405', '0.5283276216093178', '0.527229687050003', '0.5254692764087077', '0.5241575336399078', '0.5263414329348908', '0.5295051418750139', '0.5282711111165478', '0.527781304716572', '0.5240534779506436', '0.5210598143720588', '0.5209739090340898', '0.5173124963530058', '0.5087753986457796', '0.5094542984575778', '0.5063584640132204', '0.5069941968742933', '0.5067807793767996', '0.5057228847874429', '0.5054683163902676', '0.506152228622091', '0.5093079321981616', '0.5096394005716756', '0.505042043419435', '0.5070519545449528', '0.510664092132816', '0.5102756698831603', '0.507007336326063', '0.5057108893942621', '0.5107148487334549', '0.5160271507160017', '0.5139136966806507', '0.5177110107619384', '0.5148649682190023', '0.5253792076114774', '0.5265364267437891', '0.5297426086379049', '0.5225341917396458', '0.5223703601886921', '0.5227738698911033', '0.5188232996016828', '0.5207789137883941', '0.5190639135856209', '0.5178515558712378', '0.5177555109002636', '0.5157120427573896', '0.5085586462657915', '0.5038046311002429', '0.504376007070243', '0.5053541403237094', '0.5054183063271849', '0.5037325654431486', '0.49627366578899246', '0.4945776490544167', '0.4926549900134788', '0.4930631658738597', '0.49341049457799335', '0.4922478140392128', '0.4922534585826228', '0.49065122521993676', '0.4894412227137419', '0.4907764096516022', '0.48865216838690095', '0.4860579902827956', '0.4846294198086726', '0.4853015344745867', '0.48875768828196253', '0.48913235795835763', '0.4904831307301052', '0.48895498945501936', '0.48758713580143487', '0.4879158955779491', '0.48547850821022764', '0.48671964836496373', '0.4854059672781761', '0.48893783797837786', '0.49308578868859965', '0.4902694749843257', '0.4942054093346304', '0.4915253669989733', '0.49008193165983327', '0.48821694171671426', '0.4865687158570877', '0.4845743261261629', '0.4834163514876245', '0.47911533891655855', '0.47595165393905514', '0.4751254604721491', '0.4724178281622555', '0.4758913974035489', '0.4761802995089649', '0.4760029824595454', '0.4716903929706001', '0.4682251762623052', '0.46634473910617247', '0.46218172001060187', '0.46374014303925937', '0.46559945026133565', '0.4679125033083482', '0.4683393176694266', '0.47235633379162545', '0.4720048976102511', '0.4705077627943838', '0.47357767735045797', '0.4745781207932015', '0.4724070481982001', '0.4687423527163268', '0.47009366449543744', '0.47437246420348134', '0.4716689558650655', '0.4716032841043619', '0.4726360018493093', '0.4742650498961364', '0.4733751134070274', '0.4725381629641029', '0.4759616670538408', '0.47491232764787494', '0.47679742136551817', '0.5325633877747921', '0.4753799890927338', '0.4742090761778389', '0.4752129999807556', '0.47333413831164384', '0.4729021551152989', '0.4667663649682534', '0.41032055084394203', '0.4680782039086119', '0.4707986029061232', '0.46788000798195417', '0.46748065887211576', '0.469460284571134', '0.46835414106671036', '0.4666414590484532', '0.4669544407967742', '0.46572516034433276', '0.46417822749291426', '0.46357190940081405', '0.4627959517105166', '0.46322266239354803', '0.4640129644265861', '0.4672939916528923', '0.46618372585958734', '0.4638288491719309', '0.46825876017834966', '0.4673860298581879']]"
# TODO CHANGE to actual folder  
_base_path = '../VRDrift/'
'''
    125 hz
    first 100 stimuli * 3 seconds each
    timedelta starts with 0 st second 300 is the right edge
    add one more time bracket for interpolation before trimming
'''
END_DELTA = utils._nobs # 300
_nStimuli = 100

nChannels = 2 # [normX, normY]

_device = utils._device

'''
    Selection are tables where for each participant
    there's an array with numbers 1-100 shuffled
    the numbers represent labels in Cifar100 dataset.
'''
def get_selection ():
    # ================
    # DATA
    # ================
    testing = True
    data_path = _base_path
    participants_set = set([_dir for _dir in os.listdir(data_path) if not _dir.startswith(".")])
    exclude_set = set(['DL', 'NH', 'NC', 'OL', 'OA', 'SM', 'TL', 'VT'])
    participants_list = list(participants_set - exclude_set)
    participants_list = [p for p in participants_list if 'try' not in p.lower()]
    if testing==True:
        participants_list = participants_list[:2] # for testing  - rm for train.
        print('Testing on participants,', participants_list)
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    # ^^ aks what are those numbers?
    # 300 observations per participant - 3sec
    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    print(_nStimuli * len(participants_list), 'total observations')

    selection = {participant:np.array(range(0, _nStimuli)) for participant in participants_list}
    for k in selection.keys():
        # shuffle each individually. inplace function. returns None
        shuffle(selection[k])
    # train set
    # 80 random observations of each participant
    cut = int(_nStimuli * .80)
    train_selection = {k:selection[k][:cut] for k in selection.keys()}
    # validation set
    # 20 random observations of each participant
    val_selection = {k:selection[k][cut:] for k in selection.keys()}

    # once upon a time there was a test selection array. Abandoned, forgotten and unused.
    # more can be found in git :)

    # save selections for future runs
    with open(utils.output_path+'train_selection.pickle', 'wb') as handle:
        pickle.dump(train_selection, handle, protocol=pickle.HIGHEST_PROTOCOL)

    with open(utils.output_path+'val_selection.pickle', 'wb') as handle:
        pickle.dump(val_selection, handle, protocol=pickle.HIGHEST_PROTOCOL)

    return train_selection, val_selection


'''
    converting from the order Unity loads files:
    0,1,10,11,...2,20,21,...99
    to
    0,1,2,3,...,99 
'''
def conv_alph_to_num(num):
    if num<2 or num>89:
        return num
    units = num%10
    tens = num//10
    if (units - tens) == 1:
        return units
    temp = tens*10 + tens + 1
    if (num > temp):
        temp += 11
    return num + (10 - temp%10)

#  _nStimuli+1?
# base path


class Participant:

    def __init__(self, part_name):
        self.part_name = part_name
        self.simset = 0
        self.img_list = []
        self.normX = np.array([-1] * (_nStimuli+1)) 
        self.normY = np.array([-1] * (_nStimuli+1)) 

    def set_simset(self):
        events_path = _base_path + self.part_name + '/Events.csv'
        events_df = pd.read_csv(events_path)
        self.simset = int(events_df.columns[1].split('_')[1])

    def set_norms(self):
        norms_path = _base_path + self.part_name + '/normXY.csv'
        norms_df = pd.read_csv(norms_path)

        for index, row in norms_df.iterrows():
                if index > (_nStimuli+1):
                    break
                # Access the values of each column in the current row
                image = conv_alph_to_num(row['image'])
                self.img_list.append(image)
                self.normX[image] = row['norm_x']
                self.normY[image] = row['norm_y']

class FullDataset(torch.utils.data.Dataset):

    def __init__(self, ix_dict, path=_base_path, sample_size=END_DELTA ):

        # ix_dict is a dictionary where key = participant's name and value = array of selected labels
        self.part_list = list(ix_dict)
        shuffle(self.part_list)
        
        # Moving ix_list from dictionary to pairs
        self.ix_list = [(participant, ix) for participant, ix_list in ix_dict.items() for ix in ix_list]
        shuffle(self.ix_list)

        self.part_dict = None 
        self.imgset = []
        self.imgset_labels = []
        self.sample_size = sample_size
        self.create_datasets()

    def create_datasets(self):

        cifar_data = torchvision.datasets.CIFAR100('./cifar100data/',train=True,download=True) 
        for i in range(len(cifar_data)):
            self.imgset.append(np.array(cifar_data[i][0]))
            self.imgset_labels.append(cifar_data[i][1])

        self.imgset = np.array(self.imgset)
        self.imgset_labels = np.array(self.imgset_labels)

        for part_name in self.part_list:

            participant = Participant(part_name)
            # change img with fucntion 
            participant.set_simset()
            participant.set_norms()
            self.part_dict[part_name] = participant

    def decode_image_from_simset_and_label(self, part_name, event_i):
        participant = self.part_dict[part_name]
        simset = participant.simset
        label = participant.img_list[event_i]
        loc = np.where(self.imgset_labels==label)[0][simset-1]
        return self.imgset[loc]
    
    def get_trajectory(self, part_name, event_i):
        participant = self.part_dict[part_name]
        label = participant.img_list[event_i]
        traj_normX = participant.normX[label][:self.sample_size]
        traj_normY = participant.normY[label][:self.sample_size]
        traj = [traj_normX, traj_normY] # TODO CHECK
        return traj

    def __len__(self):
        # 'Denotes the total number of samples'
        return len(self.ix_list)

    # event_i is a row in the participants table holding labels and time stamps
    # event_i != label
    def __getitem__(self, index):

        part, event_i = self.ix_list[index]
        joint_img = self.decode_image_from_simset_and_label(part, event_i)
        self.joint_tens = torch.tensor(joint_img).to(_device)
        
        # Picking a randome image from the current's mode (train/val) set 
        marg_part, marg_eventi = choice(self.ix_list)
        marg_img = self.decode_image_from_simset_and_label(marg_part, marg_eventi)
        self.marg_tens = torch.tensor(marg_img).to(_device)
        
        traj = self.get_trajectory(part, event_i)
        self.traj_tens = torch.tensor(traj).to(_device)

        return (self.traj_tens, self.joint_tens, self.marg_tens)

